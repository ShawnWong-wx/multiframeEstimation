{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Image Representation - Learn from NeRF<sup>1</sup>  and SRN<sup>2</sup> \n",
    "\n",
    "Thoughts from discussion with Qian   \n",
    "Minghao   \n",
    "Mar 31, 2021\n",
    "\n",
    "(SNR is the followup to DeepVoxels<sup>3</sup>)   \n",
    "1: [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934)   \n",
    "2: [Scene representation networks: Continuous 3D-structure-aware neural scene representations](https://papers.nips.cc/paper/2019/hash/b5dc4e5d9b495d0196f61d45b26ef33e-Abstract.html)   \n",
    "3: [DeepVoxels: Learning Persistent 3D Feature Embeddings](https://vsitzmann.github.io/deepvoxels/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two neural networks has similar input, output, and pipeline. The pipeline can be written as:\n",
    "\\begin{align*}\n",
    "    \\mathbf{D} &= \\mathcal{F}(\\{\\mathbf{X}\\}) \\\\\n",
    "    \\mathbf{Y} &= \\mathcal{G}(\\mathbf{D}, \\mathbf{V})\n",
    "\\end{align*}\n",
    "Where \n",
    "- $\\{\\mathbf{X}\\}$: Input, 100/many 2D images   \n",
    "- Representation $\\mathbf{D}$: one data chunk (array or weights) representing this single/class 3D object   \n",
    "- Showed output $\\{\\mathbf{Y}\\}$: 2D images of the object from given view points $\\{\\mathbf{V}\\}$   \n",
    "- $\\mathcal{F}$: can be regarded as fusion and compression\n",
    "- $\\mathcal{G}$: can be regarded as extraction and projection.\n",
    "\n",
    "SRN and NeRF has $\\mathbf{D}$ in different format, thus different $\\mathcal{F}$ and $\\mathcal{G}$. NeRF's $\\mathbf{D}$ is some weights for the network structure $\\mathcal{G}$, and its $\\mathcal{F}$ is the training process. SRN's $\\mathbf{D}$ is more complex, as it aims to work for a class of objects, where a latent vector $z$ chooses which object to render."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing \"entropy\".   \n",
    "In this section I use \"entropy\" in an informal manner, as a vague concept of how much information it contain. The numbers are not precise, just for comparision, saying that there's more/less information.\n",
    "\n",
    "The input $\\{\\mathbf{X}\\}$ has much redundancy. A human can grab a decent understanding of a 3D object if we show him/her just 3 or 4 images from different view points. The process $\\{\\mathbf{X}\\} \\rightarrow \\mathbf{D}$ is more or less an overposed problem. \n",
    "\n",
    "As a result, the representation $\\mathbf{D}$ takes much smaller disk space than input $\\{\\mathbf{X}\\}$, but of course, larger than a single frame $\\mathbf{X}$ in the input. \n",
    "\n",
    "Why this works so good, is that images from different viewpoints are closely related, but not easily convertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its not that easy to find a 2D \n",
    "\n",
    "Why $\\{\\mathbf{X}\\}$ and $\\{\\mathbf{Y}\\}$ are all images? 2D sensor, 2D display. I bet if we have commonly fetched 3D display, the shown thing won't be images.\n",
    "\n",
    "Currrently, all visual algorithms give \"to-eye\" results as 2D RGB pixel maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

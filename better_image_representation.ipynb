{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Image Representation - Learn from NeRF<sup>1</sup>  and SRN<sup>2</sup> \n",
    "\n",
    "Thoughts from discussion with Qian   \n",
    "Minghao   \n",
    "Mar 31, 2021\n",
    "\n",
    "(SNR is the followup to DeepVoxels<sup>3</sup>)   \n",
    "1: [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934)   \n",
    "2: [Scene representation networks: Continuous 3D-structure-aware neural scene representations](https://papers.nips.cc/paper/2019/hash/b5dc4e5d9b495d0196f61d45b26ef33e-Abstract.html)   \n",
    "3: [DeepVoxels: Learning Persistent 3D Feature Embeddings](https://vsitzmann.github.io/deepvoxels/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeRF and SRN: implicit 3D representation.\n",
    "\n",
    "These two neural networks has similar input, output, and pipeline. The pipeline can be written as:\n",
    "\\begin{align*}\n",
    "    \\mathbf{D} &= \\mathcal{F}(\\{\\mathbf{X}\\}) \\\\\n",
    "    \\mathbf{Y} &= \\mathcal{G}(\\mathbf{D}, \\mathbf{V})\n",
    "\\end{align*}\n",
    "Where \n",
    "- $\\{\\mathbf{X}\\}$: Input, 100/many 2D images   \n",
    "- $\\mathbf{D}$: Representation, one data chunk (array or weights) representing this single/class 3D object   \n",
    "- $\\{\\mathbf{Y}\\}$: Shown output, 2D images of the object from given view points $\\{\\mathbf{V}\\}$   \n",
    "- $\\mathcal{F}$: can be regarded as fusion and compression\n",
    "- $\\mathcal{G}$: can be regarded as extraction and projection.\n",
    "\n",
    "Like shown in this DeepVoxels' demostration image. $\\{\\mathbf{X}\\}$ are the images of the vase taken from different viewpoints, shown in top-left. $\\mathbf{D}$ is drawn as a colorful cube. With given viewpoints $\\{\\mathbf{V}\\}$, output $\\{\\mathbf{Y}\\}$ are rendered from $\\mathbf{D}$, as shown in bottom-right.\n",
    "![DeepVoxels](./imgs/mh_image_representation/DeepVoxels_sketch.png)\n",
    "\n",
    "SRN and NeRF has $\\mathbf{D}$ in different format, thus different $\\mathcal{F}$ and $\\mathcal{G}$.    \n",
    "NeRF's $\\mathbf{D}$ is some weights for the network structure $\\mathcal{G}$, and its $\\mathcal{F}$ is a training process.   \n",
    "SRN's $\\mathbf{D}$ is more complex, as it aims to work for a class of objects, where a latent vector $z$ chooses which object to render.\n",
    "\n",
    "The input $\\{\\mathbf{X}\\}$ has much redundancy. A human can grab a decent understanding of a 3D object if we show him/her just 3 or 4 images from different view points. The process $\\{\\mathbf{X}\\} \\rightarrow \\mathbf{D}$ is more or less an overposed problem. As a result, the representation $\\mathbf{D}$ takes even smaller disk space than input $\\{\\mathbf{X}\\}$.\n",
    "\n",
    "The shown output $\\{\\mathbf{Y}\\}$ can be regarded as projections of the 3D object, and it lies in the same space of input $\\{\\mathbf{X}\\}$. This makes it easier to train the network, as we may split a set of $\\{\\mathbf{X}\\}$ to a input and groundtruth data.\n",
    "\n",
    "By the way, I notice that $\\{\\mathbf{X}\\}$ and $\\{\\mathbf{Y}\\}$ are all 2D images. Why? My opinion is that this is the restriction of our sensor and display. We have 2D sensors, we hav 2D displays. I bet that the shown output won't be images, if we have commercial available 3D displays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight forward 2D analogy\n",
    "\n",
    "However, it's not that easy to find a proper analogy for 2D images. \n",
    "\n",
    "As in SRN, it manages to find a function $\\Phi: (x,y,z) \\rightarrow \\mathbf{v}$, where $(x,y,z)$ is a 3D coordinate, and $\\mathbf{v}$ is a representation vector containing color and radiance information. Then it uses $\\Phi$ to derive $\\mathcal{G}$.   \n",
    "NeRF works similarly, but trace values along rays, not points.  \n",
    "\n",
    "Of course we can try the most straight forward analogy, finding a function $\\Phi: (x,y) \\rightarrow \\mathbf{v}$. Similar method has been raised by CPPN<sup>1</sup>, which uses neural network to perform a function $\\Phi: (x,y) \\rightarrow v$. That paper use it as a pattern generator, instead of a representation or compression. I'm still scanning papers cited CPPN.  \n",
    "Anyway, we can train such a network, and it should be a reasonable inpainting algorithm, as we can train on existing pixels, and predict missing pixels.    \n",
    "However, though we may make arbitrary dense $(x,y)$ grid, I don't think it would easily handle super-resolution tasks. As mentioned in SRN paper, they tried to make denser $(x,y,z)$ grid, but the resulting voxel array didn't contain much high frequency information beyond input images.\n",
    "\n",
    "A main difference between the 3D algorithms and this straight forward 2D analogy, in my opinion, is the relation between representation $\\mathbf{D}$ and shown output $\\{\\mathbf{Y}\\}$.   \n",
    "In the 3D algorithms, with a specific viewpoint $\\mathbf{V}$, we can project $\\mathbf{D}$ to $\\mathbf{Y}$. The reverse projection is very hard. What's more, though different $\\mathbf{Y}$s are closely correlated, its hard to infer one from another.   \n",
    "In the 2D analogy, the output a 2D pixel map is itself a fuction $\\mathbf{I}: (x,y) \\rightarrow v$ with band-limited output,  which is almost identical to the proposed representation $\\Phi: (x,y) \\rightarrow v$. What's more, the output can be fully represented with just one image, not a set of images. Thus, there's not much superiority in using a function to represent an image. \n",
    "\n",
    "1: [Compositional Pattern Producing Networks: A Novel Abstraction of Development](https://link.springer.com/content/pdf/10.1007/s10710-007-9028-8.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand a 2D image\n",
    "\n",
    "A possible way out is to \"expand\" a 2D image. \n",
    "\n",
    "For example, we can add color channels. We can estimate a implicit representation $\\Phi: (x,y, \\lambda) \\rightarrow \\mathbf{v}$ from multiframe input or CASSI-style input.    \n",
    "In this way, the images with different $\\lambda$ are closely correlated, but not easily transfered. The implicit representation $\\Phi$ can easily project to a image with $\\lambda$, and the reverse is hard. That should be a better analogy.   \n",
    "I don't know if there's already paper about it. I'm searching them.\n",
    "\n",
    "Of course we can think of more ways to expand an image. A existing example estimating BTF (Bidirectional Texture Function) with neural network<sup>1</sup>. I'm still thinking for more options.\n",
    "\n",
    "1: [Neural BTF Compression and Interpolation](http://rgl.epfl.ch/publications/Rainer2019Neural)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
